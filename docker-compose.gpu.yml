version: '3.8'

services:
  llm-server:
    build:
      context: .
      dockerfile: Dockerfile
    image: llm-gpu-server:latest
    container_name: llm-dynamic-gpu
    restart: unless-stopped
    environment:
      - SERVER_PORT=8001
      - GPU_LAYERS=32
      - CONTEXT_SIZE=4096
      - MAX_VRAM_GB=11
      - DEFAULT_MODEL=llama
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "8001:8001"
    volumes:
      - ./models:/app/models
      - ./config:/app/config
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: 2gb

  gateway:
    build:
      context: ./gateway
      dockerfile: Dockerfile
    image: llm-gateway:latest
    container_name: llm-gateway
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - LLM_SERVER_URL=http://llm-server:8001
      - MAX_TOKENS_LIMIT=1024
      - ENABLE_MODEL_SWITCHING=true
    depends_on:
      - llm-server
